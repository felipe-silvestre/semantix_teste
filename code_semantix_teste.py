# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Y1R39XSqij6WyUNlTtpRVg9zxCXpf0P
"""

pip install pyspark

pip install datetime

from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql import Row

"""file_path = caminho dos arquivos txt"""

def load_file(sc,file_path):
    return sc.textFile(file_path)

def organize_data_into_dataframe(data_rdd):
    from datetime import datetime
    
    invalid_data_list = [
                        "Invalid_data",
                        "-",
                        "-",
                        datetime.now().strftime("%d/%b/%Y"),
                        "-",
                        "-",
                        "-",
                        "0"
                        ]

    final_df = data_rdd.map(lambda line: line.split(" ") if len(line.split(" ")) >= 8
                                                         else invalid_data_list) \
                       .map(lambda line: Row(host=line[0], 
                       timestamp=datetime.strptime(str(line[3]).replace('[',''), "%d/%b/%Y"),
                       requestMethod=line[5].encode('utf-8').replace('"',''),
                       request=line[6].encode('utf-8').replace('"',''),
                       HTTPcode=line[-2], 
                       totalBytes=int(line[-1].encode('utf-8').replace('-', '0')))).toDF()

    return final_df

def data_frame_debug(df):
    df.printSchema() 
    df.show() 

def hosts_distintos(spark):
    query = """
    SELECT count(DISTINCT Host) as HostsQuantity
    FROM logs_table
    """
    return spark.sql(query)

def erro_404(spark):
    query = """
    SELECT count(HTTPcode) as PagesNotFound
    FROM logs_table
    WHERE HTTPcode = '404'
    """

    return spark.sql(query)

def erro_404_top_url(spark):
    query = """
    SELECT concat(Host, Request) as URL,
           count(HTTPcode) as PagesNotFound
    FROM logs_table
    WHERE HTTPcode = '404'
    GROUP BY URL
    ORDER BY PagesNotFound DESC LIMIT 5
    """

    return spark.sql(query)

def erro_404_por_dia(spark):
    query = """
    SELECT cast(timestamp as date) as Date,
           count(HTTPcode) as PagesNotFound
    FROM logs_table
    WHERE HTTPcode = '404'
    GROUP BY Date
    """

    return spark.sql(query)

def get_total_bytes(spark):
    query = """
    SELECT sum(totalBytes) as TotalBytes
    FROM logs_table
    """

    return spark.sql(query)

def main():
    from pandas import DataFrame

sc = SparkContext()
    spark = SparkSession(sc)

    # arquivos
    log_ago95 = load_file(sc,"access_log_Aug95")
    log_jul95 = load_file(sc,"access_log_Jul95")

    # Converting arquivo em dataframe
    log_ago95_DF = organize_data_into_dataframe(log_ago95)
    log_jul95_DF = organize_data_into_dataframe(log_jul95)

    # unificando dados
    logs_df = log_ago95_DF.union(log_jul95_DF)
    
    #Limpando linhas inválidas e registrando dados válidos como temporária
    logs_df.where("host != 'Invalid_data'").registerTempTable("logs_table")

    # Número de hosts únicos
    hosts_unicos_df = hosts_distintos(spark)
    hosts_unicos_df.toPandas().to_csv('hosts_unicos.csv')

    # O total de erros 404
    erro_404_contagem_df = erro_404(spark)
    erro_404_contagem_df.toPandas().to_csv('total_erros_404.csv')

    # Os 5 URLs que mais causaram erro 404
    erro_404_top_url_df = erro_404_top_url(spark)
    erro_404_top_url_df.toPandas().to_csv('top_urls_error_404.csv')

    # Quantidade de erros 404 por dia.
    erro_404_por_dia = erro_404_por_dia(spark)
    erro_404_por_dia.toPandas().to_csv('erro_404_por_dia.csv')

    # O total de bytes retornados
    total_bytes_df = get_total_bytes(spark)
    total_bytes_df.toPandas().to_csv('total_bytes.csv')

main()